{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed-Sohail2000/Satellite-Image-Analysis-with-Stable-Diffusion-YOLOv8-/blob/main/diffusionmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UCI7M3k4MN"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers accelerate transformers peft torchvision\n",
        "!pip install pytorch-fid open-clip-torch matplotlib seaborn zipfile36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtfTZsKjnKXt"
      },
      "outputs": [],
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install accelerate torch tqdm --quiet\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "from diffusers import (StableDiffusionImg2ImgPipeline,\n",
        "                       UNet2DConditionModel,\n",
        "                       AutoencoderKL)\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPFeatureExtractor\n",
        "from diffusers import DDIMScheduler\n",
        "import numpy as np\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "\n",
        "# For metrics calculation\n",
        "import open_clip\n",
        "from pytorch_fid import fid_score\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7uC55otkCe_"
      },
      "outputs": [],
      "source": [
        "if os.path.exists('data1.zip'):\n",
        "    print(\"Extracting data1.zip...\")\n",
        "    with zipfile.ZipFile('data1.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    print(\"Extraction complete!\")\n",
        "else:\n",
        "    print(\"data.zip not found. Please upload the file first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jZ0F9hbkF9F"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(\"Checking data1 structure...\")\n",
        "# if os.path.exists('data1'):\n",
        "#     print(\"✓ data1 folder found\")\n",
        "#     if os.path.exists('data1/data'):  # CORRECTED: data1/data\n",
        "#         print(\"✓ data1/data folder found\")\n",
        "tif_files = [f for f in os.listdir('data1') if f.endswith('.tif')]\n",
        "# jpg_files = [f for f in os.listdir('data1') if f.endswith('.jpg')]\n",
        "#         # print(f\"✓ Found {len(tif_files)} .tif files\")\n",
        "#         print(f\"✓ Found {len(jpg_files)} .jpg files\")\n",
        "#     else:\n",
        "#         print(\"✗ data1/data folder not found\")\n",
        "# else:\n",
        "#     print(\"✗ data1 folder not found\")\n",
        "\n",
        "# List first few files to verify\n",
        "if os.path.exists('data1'):  # CORRECTED: data1/data\n",
        "    print(\"\\nFirst 5 files in data1:\")\n",
        "    files = os.listdir('data1')[:5]\n",
        "    for f in files:\n",
        "        print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWb6REM1nFiW"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'dataset_folder': 'data1',  # Main dataset folder\n",
        "    'output_dir': 'stable_diffusion_output',\n",
        "    'model_id': 'stabilityai/stable-diffusion-2-1',\n",
        "    'num_epochs': 5,\n",
        "    'batch_size': 1,\n",
        "    'learning_rate': 1e-4,\n",
        "    'image_size': 512,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "print(f\"Using device: {CONFIG['device']}\")\n",
        "print(f\"Dataset folder: {CONFIG['dataset_folder']}\")\n",
        "\n",
        "######################################\n",
        "# Part 1: Enhanced Dataset Class\n",
        "######################################\n",
        "\n",
        "class SatelliteDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None, use_tif=True):\n",
        "        self.folder = folder\n",
        "        # Use .tif files for better quality satellite images\n",
        "        if use_tif:\n",
        "            self.image_files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
        "                                if f.lower().endswith('.tif')]\n",
        "        else:\n",
        "            self.image_files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        print(f\"Found {len(self.image_files)} images in {folder}\")\n",
        "\n",
        "        self.transform = transform if transform is not None else T.Compose([\n",
        "            T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.5]*3, [0.5]*3)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_files[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJTF9MMlEy2I"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(dataset_folder=\"data1/data\",\n",
        "                    output_dir=\"fine_tuned_satellite_model\",\n",
        "                    num_epochs=5,\n",
        "                    batch_size=1,\n",
        "                    lr=1e-4):\n",
        "    device = CONFIG['device']\n",
        "    model_id = CONFIG['model_id'] # Use model_id from CONFIG\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"metrics\"), exist_ok=True)\n",
        "\n",
        "    print(\"Loading base VAE and UNet...\")\n",
        "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
        "\n",
        "    # Set up LoRA for parameter-efficient fine-tuning\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "        lora_dropout=0.05,\n",
        "    )\n",
        "    unet = get_peft_model(unet, lora_config)\n",
        "    unet.print_trainable_parameters()\n",
        "\n",
        "    # Enable gradient checkpointing\n",
        "    unet.enable_gradient_checkpointing()\n",
        "\n",
        "    vae.to(device)\n",
        "    unet.to(device)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    transform = T.Compose([\n",
        "        T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.5]*3, [0.5]*3)\n",
        "    ])\n",
        "    dataset = SatelliteDataset(dataset_folder, transform=transform, use_tif=True)\n",
        "\n",
        "    # Split dataset for training/validation\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=lr)\n",
        "\n",
        "    # FIXED: Use new GradScaler syntax\n",
        "    if device == 'cuda':\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "    else:\n",
        "        scaler = torch.amp.GradScaler('cpu')\n",
        "\n",
        "    # Metrics tracking\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    print(\"Starting fine-tuning on satellite images...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        unet.train()\n",
        "        epoch_train_loss = 0.0\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # FIXED: Use new autocast syntax\n",
        "            if device == 'cuda':\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    latents = vae.encode(batch).latent_dist.sample() * 0.18215\n",
        "                    timesteps = torch.randint(0, 1000, (batch.size(0),), device=device)\n",
        "                    # FIXED: Correct the dummy text embedding size to 1024 for stable-diffusion-2-1\n",
        "                    dummy_text_embeddings = torch.zeros((batch.size(0), 77, 1024), device=device)\n",
        "                    noise_pred = unet(latents, timesteps, encoder_hidden_states=dummy_text_embeddings).sample\n",
        "                    loss = torch.nn.functional.mse_loss(noise_pred, latents)\n",
        "            else:\n",
        "                latents = vae.encode(batch).latent_dist.sample() * 0.18215\n",
        "                timesteps = torch.randint(0, 1000, (batch.size(0),), device=device)\n",
        "                # FIXED: Correct the dummy text embedding size to 1024 for stable-diffusion-2-1\n",
        "                dummy_text_embeddings = torch.zeros((batch.size(0), 77, 1024), device=device)\n",
        "                noise_pred = unet(latents, timesteps, encoder_hidden_states=dummy_text_embeddings).sample\n",
        "                loss = torch.nn.functional.mse_loss(noise_pred, latents)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            epoch_train_loss += loss.item()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation\n",
        "        unet.eval()\n",
        "        epoch_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
        "                batch = batch.to(device)\n",
        "\n",
        "                # FIXED: Use new autocast syntax\n",
        "                if device == 'cuda':\n",
        "                    with torch.amp.autocast('cuda'):\n",
        "                        latents = vae.encode(batch).latent_dist.sample() * 0.18215\n",
        "                        timesteps = torch.randint(0, 1000, (batch.size(0),), device=device)\n",
        "                        # FIXED: Correct the dummy text embedding size to 1024 for stable-diffusion-2-1\n",
        "                        dummy_text_embeddings = torch.zeros((batch.size(0), 77, 1024), device=device)\n",
        "                        noise_pred = unet(latents, timesteps, encoder_hidden_states=dummy_text_embeddings).sample\n",
        "                        loss = torch.nn.functional.mse_loss(noise_pred, latents)\n",
        "                else:\n",
        "                    latents = vae.encode(batch).latent_dist.sample() * 0.18215\n",
        "                    timesteps = torch.randint(0, 1000, (batch.size(0),), device=device)\n",
        "                    # FIXED: Correct the dummy text embedding size to 1024 for stable-diffusion-2-1\n",
        "                    dummy_text_embeddings = torch.zeros((batch.size(0), 77, 1024), device=device)\n",
        "                    noise_pred = unet(latents, timesteps, encoder_hidden_states=dummy_text_embeddings).sample\n",
        "                    loss = torch.nn.functional.mse_loss(noise_pred, latents)\n",
        "\n",
        "                epoch_val_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
        "        avg_val_loss = epoch_val_loss / len(val_dataloader)\n",
        "\n",
        "        training_losses.append(avg_train_loss)\n",
        "        validation_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(training_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(validation_losses, label='Validation Loss', color='red')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"metrics\", \"training_validation_loss.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Save fine-tuned model\n",
        "    unet_save_dir = os.path.join(output_dir, \"unet\")\n",
        "    vae_save_dir = os.path.join(output_dir, \"vae\")\n",
        "    os.makedirs(unet_save_dir, exist_ok=True)\n",
        "    os.makedirs(vae_save_dir, exist_ok=True)\n",
        "\n",
        "    unet.save_pretrained(unet_save_dir)\n",
        "    vae.save_pretrained(vae_save_dir)\n",
        "\n",
        "    # Save metrics\n",
        "    metrics = {\n",
        "        'training_losses': training_losses,\n",
        "        'validation_losses': validation_losses,\n",
        "        'final_training_loss': training_losses[-1],\n",
        "        'final_validation_loss': validation_losses[-1]\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, \"metrics\", \"training_metrics.json\"), 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    print(\"Fine-tuning completed and model saved at:\", output_dir)\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4XpPnd0Fmzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "from diffusers import StableDiffusionImg2ImgPipeline, DPMSolverMultistepScheduler\n",
        "from transformers import CLIPImageProcessor, CLIPTokenizer, CLIPTextModel\n",
        "from PIL import Image, ImageEnhance\n",
        "from peft import PeftModel\n",
        "from diffusers import UNet2DConditionModel, AutoencoderKL\n",
        "\n",
        "# # Enhanced satellite prompts\n",
        "# SATELLITE_PROMPTS = [\n",
        "#     \"Sentinel-2 B08 NIR image: plastic accumulation zones in the North Pacific Gyre at 32.1°N 145.7°W, low chlorophyll concentration, strong reflectance at 842 nanometers, water-leaving radiance > 5 W/m²/sr/μm, microplastic index over 0.6, no cloud contamination, minimal wave interference\",\n",
        "\n",
        "#     \"Landsat-9 surface reflectance image of estuarine litter patch at 29.8°N 90.1°W, Mississippi Delta, plastic-dominated flotsam in turbid outflow, reflectance anomaly at 1650–1750 nm, suspended solids concentration 80 mg/L, anthropogenic signature confirmed via spectral unmixing\",\n",
        "\n",
        "#     \"Sentinel-3 OLCI image: South China Sea at 12.6°N 110.2°E, foam-plastic mixture observed on coastal shelf edge, visible and NIR bands showing high scattering, sunglint conditions moderate, plastic polymer absorption bands near 1215 and 1735 nm detected with 80% confidence\",\n",
        "\n",
        "#     \"Multispectral satellite image (WorldView-3): urban river segment in Jakarta at 6.2°S 106.8°E, accumulated macroplastic raft near bridge structure, multispectral plastic index exceeding 0.7, solar zenith angle 47°, shadow correction applied, high-resolution panchromatic band confirms debris geometry\",\n",
        "\n",
        "#     \"MODIS-Aqua 250m resolution: equatorial upwelling zone at 0.8°N 134.9°W, dispersed synthetic particles with specular reflection hotspots, red-edge reflectance peak at 710 nm, normalized difference plastic index (NDPI) > 0.5, concurrent SST anomaly of +1.2°C, cloud mask applied\"\n",
        "# ,\n",
        "#         \"Sentinel-2 B11 SWIR image: clear ocean surface at 23.6°S 46.5°W with plastic debris field , 0.5 kilometer dispersion index, buoyant fraction greater than 80%, spectral signature at 1215 nanometers, sunglint, NIR to SWIR ratio greater than 1.7, atmospheric correction 6S, no ships or cloud cover\",\n",
        "#     \"Plastic debris field floating on clear ocean surface viewed from satellite perspective, South Atlantic waters, scattered white and colored fragments across blue expanse, 500-meter dispersal pattern, bright sunglint reflections\"\n",
        "# ,\n",
        "#     \"Landsat-8 mixed litter in river plume outflow at 8.4°S 13.2°E, turbidity 15 NTU , colored dissolved organic matter absorption 0.2 per meter, diagnostic spectral features: polyethylene at 1660 nanometers, polyvinyl chloride at 1735 nanometers\"\n",
        "\n",
        "# ]\n",
        "SATELLITE_PROMPTS =\n",
        " [\"Realistic Sentinel-2 satellite image showing river plume discharging plastic waste into ocean, 10m resolution with visible debris lines following currents, natural color rendering\"\n",
        ",\"High-detail Copernicus satellite view of a realistic accumulation of mixed marine debris in the Pacific Ocean, 20 meter spatial resolution, no cloud contamination, minimal wave interference\"\n",
        ",\"Realistic Landsat-8 of mixed marine plastics in river plume outflow 5 meter spatial resolution, bright sunlight reflections, no ships or cloud cover \"\n",
        ",\"Satellite view of ocean plastic pollution near coastlines, visible microplastics and trash patches, remote sensing of marine pollution\"\n",
        ",\"Satellite view of ocean plastic pollution streaks of macroplastics and trash patches, 30meter spatial resolution remote sensing of marine pollution\"\n",
        ",\"Realistic satellite view of ocean plastic pollution, near coastline, 10meter spatial resolution, sunglint\"]\n",
        "\n",
        "\n",
        "class SatelliteImageGenerator:\n",
        "    def __init__(self, fine_tuned_model_dir):\n",
        "        device = CONFIG['device']\n",
        "        unet_path = os.path.join(fine_tuned_model_dir, \"unet\")\n",
        "        vae_path = os.path.join(fine_tuned_model_dir, \"vae\")\n",
        "\n",
        "        print(\"Loading model... (This will only happen once)\")\n",
        "        # Load the base UNet from the same model_id used for fine-tuning\n",
        "        base_unet = UNet2DConditionModel.from_pretrained(CONFIG['model_id'], subfolder=\"unet\")\n",
        "        unet = PeftModel.from_pretrained(base_unet, unet_path)\n",
        "        vae = AutoencoderKL.from_pretrained(vae_path)\n",
        "\n",
        "        # FIXED: Load CLIPTextModel and CLIPTokenizer from stable-diffusion-2-1-base\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"tokenizer\")\n",
        "        text_encoder = CLIPTextModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"text_encoder\")\n",
        "\n",
        "        scheduler = DPMSolverMultistepScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\", subfolder=\"scheduler\")\n",
        "        feature_extractor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "        self.pipe = StableDiffusionImg2ImgPipeline(\n",
        "            unet=unet,\n",
        "            vae=vae,\n",
        "            tokenizer=tokenizer,\n",
        "            text_encoder=text_encoder,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "            safety_checker=None,\n",
        "            requires_safety_checker=False\n",
        "        ).to(device)\n",
        "\n",
        "    def generate_image(self, input_image_path, prompt=None, target_width=1024, target_height=1024,\n",
        "                       strength=0.7, guidance_scale=7.5, num_inference_steps=30, seed=None):\n",
        "        device = CONFIG['device']\n",
        "\n",
        "        init_image = Image.open(input_image_path).convert(\"RGB\")\n",
        "        init_image = init_image.resize((512, 512))\n",
        "\n",
        "        enhancer = ImageEnhance.Brightness(init_image)\n",
        "        init_image = enhancer.enhance(random.uniform(0.8, 1.2))\n",
        "        init_image = init_image.rotate(random.choice([0, 90, 180, 270]))\n",
        "\n",
        "        if prompt is None:\n",
        "            prompt = random.choice(SATELLITE_PROMPTS)\n",
        "\n",
        "        generator = torch.manual_seed(seed) if seed else None\n",
        "        result = self.pipe(\n",
        "            prompt=prompt,\n",
        "            image=init_image,\n",
        "            strength=random.uniform(0.4, 0.6),\n",
        "            guidance_scale=random.uniform(6.5, 8.5),\n",
        "            height=target_height,\n",
        "            width=target_width,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            generator=generator\n",
        "        )\n",
        "\n",
        "        return result.images[0]\n",
        "\n",
        "    def generate_batch_images(self, input_image_path, batch_size, output_dir):\n",
        "        \"\"\"Generate a specific batch size of images\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Generating {batch_size} images in {output_dir}...\")\n",
        "\n",
        "        for i in tqdm(range(batch_size), desc=f\"Generating batch of {batch_size}\"):\n",
        "            generated_image = self.generate_image(\n",
        "                input_image_path=input_image_path,\n",
        "                prompt=random.choice(SATELLITE_PROMPTS),\n",
        "                strength=random.uniform(0.6, 0.9),\n",
        "                guidance_scale=random.uniform(6.5, 8.5),\n",
        "                num_inference_steps=random.randint(40, 60),\n",
        "                seed=random.randint(0, 10000)\n",
        "            )\n",
        "\n",
        "            output_path = os.path.join(output_dir, f\"generated_image_{i+1:04d}.png\")\n",
        "            generated_image.save(output_path)\n",
        "\n",
        "        print(f\"Generated {batch_size} images in {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKjW8r6-ik0J"
      },
      "outputs": [],
      "source": [
        "class MetricsCalculator:\n",
        "    def __init__(self):\n",
        "        self.device = CONFIG['device']\n",
        "        # Load CLIP model for CLIP score calculation\n",
        "        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
        "            'ViT-B-32', pretrained='openai'\n",
        "        )\n",
        "        self.clip_model.to(self.device)\n",
        "\n",
        "    def calculate_clip_score(self, generated_images, prompts):\n",
        "\n",
        "        \"\"\"Calculate CLIP score between generated images and prompts\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        for img, prompt in zip(generated_images, prompts):\n",
        "\n",
        "            # Preprocess image\n",
        "            img_tensor = self.clip_preprocess(img).unsqueeze(0).to(self.device)\n",
        "\n",
        "            # Encode image and text\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(img_tensor)\n",
        "                text_features = self.clip_model.encode_text(\n",
        "                    open_clip.tokenize([prompt]).to(self.device)\n",
        "                )\n",
        "\n",
        "                # Normalize features\n",
        "                image_features = F.normalize(image_features, dim=-1)\n",
        "                text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "                # Calculate cosine similarity\n",
        "                score = (image_features * text_features).sum(dim=-1).item()\n",
        "                scores.append(score)\n",
        "\n",
        "        return np.mean(scores), scores\n",
        "\n",
        "    def calculate_fid_score(self, real_images_dir, generated_images_dir):\n",
        "        \"\"\"Calculate FID score between real and generated images\"\"\"\n",
        "        try:\n",
        "            fid_value = fid_score.calculate_fid_given_paths(\n",
        "                [real_images_dir, generated_images_dir],\n",
        "                batch_size=50,\n",
        "                device=self.device\n",
        "            )\n",
        "            return fid_value\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating FID: {e}\")\n",
        "            return None\n",
        "\n",
        "def create_metrics_visualizations(metrics_data, output_dir):\n",
        "    \"\"\"Create visualization plots for all metrics\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # CLIP Score visualization\n",
        "    if 'clip_scores' in metrics_data:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(metrics_data['clip_scores'])\n",
        "        plt.xlabel('Image Index')\n",
        "        plt.ylabel('CLIP Score')\n",
        "        plt.title('CLIP Scores for Generated Images')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(output_dir, 'clip_scores.png'))\n",
        "        plt.show()\n",
        "\n",
        "    # FID Score comparison\n",
        "    if 'fid_scores' in metrics_data:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        batch_sizes = list(metrics_data['fid_scores'].keys())\n",
        "        fid_values = list(metrics_data['fid_scores'].values())\n",
        "        plt.bar(batch_sizes, fid_values)\n",
        "        plt.xlabel('Batch Size')\n",
        "        plt.ylabel('FID Score')\n",
        "        plt.title('FID Scores for Different Batch Sizes')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(os.path.join(output_dir, 'fid_scores.png'))\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMYj_IJkcDo6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "input_image_path = 'Input_image_satellite.png' # Replace with your desired path\n",
        "\n",
        "if os.path.exists(input_image_path):\n",
        "    print(f\"The file '{input_image_path}' exists.\")\n",
        "else:\n",
        "    print(f\"The file '{input_image_path}' does not exist.\")"
      ],
      "metadata": {
        "id": "mN2fop9sZ9yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJjl3hZsimC1"
      },
      "outputs": [],
      "source": [
        "# # Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "# pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "# neg_prompt = \"((white astronaut suit)), (((distorted limbs))), (((bad proportions))), ((extra limbs))\"\n",
        "\n",
        "# images = pipe(prompt, negative_prompt=neg_prompt).images[0]\n",
        "\n",
        "\n",
        "def run_stable_diffusion_pipeline(input_image_path=None):\n",
        "    \"\"\"Complete pipeline\"\"\"\n",
        "\n",
        "    # Step 1: Fine-tune the model\n",
        "    print(\"=== Step 1: Fine-tuning Stable Diffusion ===\")\n",
        "    fine_tuned_dir = fine_tune_model(\n",
        "        dataset_folder=CONFIG['dataset_folder'],\n",
        "        num_epochs=CONFIG['num_epochs']\n",
        "    )\n",
        "\n",
        "    # Step 2: Initialize generator\n",
        "    print(\"=== Step 2: Initializing Image Generator ===\")\n",
        "    generator = SatelliteImageGenerator(fine_tuned_dir)\n",
        "\n",
        "    # Step 3: Generate batches\n",
        "    print(\"=== Step 3: Generating Image Batches ===\")\n",
        "    batch_sizes = [10]\n",
        "\n",
        "    # Use first image as input for generation if no specific path is provided\n",
        "    if input_image_path is None:\n",
        "        dataset = SatelliteDataset(CONFIG['dataset_folder'], use_tif=True)\n",
        "        if dataset.image_files:\n",
        "            input_image_path = dataset.image_files[0]\n",
        "            print(f\"Using first image from dataset as input: {input_image_path}\")\n",
        "        else:\n",
        "            raise ValueError(f\"No image files found in the dataset folder: {CONFIG['dataset_folder']}\")\n",
        "    else:\n",
        "        if not os.path.exists(input_image_path):\n",
        "            raise FileNotFoundError(f\"Input image not found at: {input_image_path}\")\n",
        "        print(f\"Using specified input image: {input_image_path}\")\n",
        "\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        output_dir = os.path.join(CONFIG['output_dir'], f\"batch_{batch_size}\")\n",
        "        generator.generate_batch_images(input_image_path, batch_size, output_dir)\n",
        "\n",
        "    # Step 4: Calculate metrics\n",
        "    print(\"=== Step 4: Calculating Diversity Metrics ===\")\n",
        "    metrics_calc = MetricsCalculator()\n",
        "    metrics_data = {\n",
        "        'clip_scores': {},\n",
        "        'fid_scores': {}\n",
        "    }\n",
        "\n",
        "    # Calculate FID scores for each batch\n",
        "    for batch_size in batch_sizes:\n",
        "        generated_dir = os.path.join(CONFIG['output_dir'], f\"batch_{batch_size}\")\n",
        "        fid_score = metrics_calc.calculate_fid_score(CONFIG['dataset_folder'], generated_dir)\n",
        "        if fid_score:\n",
        "            metrics_data['fid_scores'][batch_size] = fid_score\n",
        "\n",
        "    # Calculate CLIP scores for a sample of generated images\n",
        "    sample_size = 200\n",
        "    for batch_size in batch_sizes:\n",
        "        generated_dir = os.path.join(CONFIG['output_dir'], f\"batch_{batch_size}\")\n",
        "        generated_files = [f for f in os.listdir(generated_dir) if f.endswith('.png')][:sample_size]\n",
        "\n",
        "        generated_images = []\n",
        "        prompts = []\n",
        "\n",
        "        for file in generated_files:\n",
        "            img_path = os.path.join(generated_dir, file)\n",
        "            img = Image.open(img_path)\n",
        "            generated_images.append(img)\n",
        "            prompts.append(random.choice(SATELLITE_PROMPTS))\n",
        "\n",
        "        clip_score, clip_scores = metrics_calc.calculate_clip_score(generated_images, prompts)\n",
        "\n",
        "        metrics_data['clip_scores'][batch_size] = {\n",
        "            'mean': clip_score,\n",
        "            'individual': clip_scores\n",
        "        }\n",
        "\n",
        "    # Step 5: Create visualizations\n",
        "    print(\"=== Step 5: Creating Visualizations ===\")\n",
        "    create_metrics_visualizations(metrics_data, os.path.join(CONFIG['output_dir'], 'metrics'))\n",
        "\n",
        "    # Step 6: Save final metrics\n",
        "    with open(os.path.join(CONFIG['output_dir'], 'metrics', 'final_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "    # Step 7: Create zip files\n",
        "    print(\"=== Step 6: Creating Output Zip Files ===\")\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        batch_dir = os.path.join(CONFIG['output_dir'], f\"batch_{batch_size}\")\n",
        "        zip_path = os.path.join(CONFIG['output_dir'], f\"stable_diffusion_batch_{batch_size}_{timestamp}.zip\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "            for root, dirs, files in os.walk(batch_dir):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, batch_dir)\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "\n",
        "    print(f\"Output directory: {CONFIG['output_dir']}\")\n",
        "    print(\"Generated:\")\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"  - {batch_size} images in batch_{batch_size}/\")\n",
        "        print(f\"  - Zip file: stable_diffusion_batch_{batch_size}_{timestamp}.zip\")\n",
        "    print(\"  - Metrics and visualizations in metrics/\")\n",
        "\n",
        "# Run the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    run_stable_diffusion_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJERQmusiokO"
      },
      "outputs": [],
      "source": [
        "# Display sample generated images\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def display_sample_images(batch_size=50, num_samples=5):\n",
        "    image_dir = os.path.join(CONFIG['output_dir'], f\"batch_{batch_size}\")\n",
        "\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"Directory {image_dir} not found. Run the pipeline first.\")\n",
        "        return\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')][:num_samples]\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
        "\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        try:\n",
        "            img = plt.imread(image_path)\n",
        "            axes[i].imshow(img)\n",
        "            axes[i].set_title(f\"Generated {i+1}\")\n",
        "            axes[i].axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying image {image_file}: {e}\")\n",
        "            axes[i].set_title(f\"Error loading {image_file}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display sample images from batch 200\n",
        "display_sample_images(batch_size=50, num_samples=5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}